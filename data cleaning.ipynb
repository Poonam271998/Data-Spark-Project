{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pre-processing using Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Exchange Rates data and convert the 'Date' column to datetime format\n",
    "df = pd.read_csv(r\"C:\\Users\\plangote\\OneDrive - DXC Production\\Desktop\\dataspark project\\Exchange_Rates.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exchange_rates_df = df  # Store the cleaned Exchange Rates DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stores data, convert 'Open Date' to datetime format, and fill NaN values with 0\n",
    "df = pd.read_csv(r\"C:\\Users\\plangote\\OneDrive - DXC Production\\Desktop\\dataspark project\\Stores.csv\")\n",
    "df['Open Date'] = pd.to_datetime(df['Open Date'])\n",
    "df = df.fillna(0)  # For online stores, square meters column is NaN, so set it to 0\n",
    "Stores_df = df  # Store the cleaned Stores DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Products data, clean 'Unit Cost USD' and 'Unit Price USD' columns by removing '$' and ',' and converting to float\n",
    "df = pd.read_csv(r\"C:\\Users\\plangote\\OneDrive - DXC Production\\Desktop\\dataspark project\\Products.csv\")\n",
    "df['Unit Cost USD'] = df['Unit Cost USD'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "df['Unit Price USD'] = df['Unit Price USD'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "Products_df = df  # Store the cleaned Products DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Customers data, handle missing state codes, and convert 'Birthday' to datetime format\n",
    "df = pd.read_csv(r\"C:\\Users\\plangote\\OneDrive - DXC Production\\Desktop\\dataspark project\\Customers.csv\", encoding='ISO-8859-1')   \n",
    "df['State Code'] = df['State Code'].fillna('Napoli')  # Fill missing state codes with 'Napoli'\n",
    "df['Birthday'] = pd.to_datetime(df['Birthday'])  # Convert 'Birthday' to datetime format\n",
    "Customers_df = df  # Store the cleaned Customers DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sales data, convert 'Order Date' and 'Delivery Date' to datetime format\n",
    "df = pd.read_csv(r\"C:\\Users\\plangote\\OneDrive - DXC Production\\Desktop\\dataspark project\\Sales.csv\")\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
    "df['Delivery Date'] = pd.to_datetime(df['Delivery Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null delivery dates with the order date plus 5 days (average delivery time)\n",
    "df.loc[df['Delivery Date'].isna(), 'Delivery Date'] = df['Order Date'] + pd.Timedelta(days=5)\n",
    "Sales_df = df  # Store the cleaned Sales DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging\n",
    "# Merge Sales data with Customers data on 'CustomerKey'\n",
    "sales_customers_df = pd.merge(Sales_df, Customers_df, on='CustomerKey', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the resulting DataFrame with Products data on 'ProductKey'\n",
    "sales_customers_products_df = pd.merge(sales_customers_df, Products_df, on='ProductKey', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Sales data with Products data on 'ProductKey'\n",
    "sales_products_df = pd.merge(Sales_df, Products_df, on='ProductKey', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Sales data with Stores data on 'StoreKey'\n",
    "sales_stores_df = pd.merge(Sales_df, Stores_df, on='StoreKey', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales_products_df with Exchange Rates data on 'Order Date' and 'Currency Code'\n",
    "Sales_products_Exchangerates_df = pd.merge(\n",
    "    sales_products_df,\n",
    "    Exchange_rates_df[['Date', 'Currency', 'Exchange']],  # Selecting only the needed columns\n",
    "    left_on=['Order Date', 'Currency Code'],  # Columns in sales_products_df\n",
    "    right_on=['Date', 'Currency'],  # Columns in exchange_rates_df\n",
    "    how='left'  # Use 'left' to keep all rows from sales_products_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in MySQL database#\n",
    "#Sales table\n",
    "import mysql.connector\n",
    "\n",
    "con = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"MynameisPBL@2710\"\n",
    ")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"create database if not exists DataSpark\")\n",
    "cursor.execute(\"use DataSpark\")\n",
    "\n",
    "query = \"\"\"CREATE TABLE if not exists sales (\n",
    "    Order_Number INT,\n",
    "    Line_Item INT,\n",
    "    Order_Date DATE,\n",
    "    Delivery_Date DATE,\n",
    "    Customer_Key INT,\n",
    "    Store_Key INT,\n",
    "    Product_Key INT,\n",
    "    Quantity INT,\n",
    "    Currency_Code VARCHAR(255)\n",
    ")\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO sales (Order_Number, Line_Item, Order_Date, Delivery_Date, Customer_Key, Store_Key, Product_Key, Quantity, Currency_Code)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame to list of tuples for insertion\n",
    "data_to_insert = Sales_df.values.tolist()\n",
    "\n",
    "# Execute insertion\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "\n",
    "# Commit and close\n",
    "con.commit()\n",
    "cursor.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Products table\n",
    "import mysql.connector\n",
    "\n",
    "con = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"MynameisPBL@2710\",\n",
    "    database=\"DataSpark\"\n",
    ")\n",
    "cursor = con.cursor()\n",
    "\n",
    "query = \"\"\"CREATE TABLE if not exists products (\n",
    "    Product_Key INT,\n",
    "    Product_Name VARCHAR(255),\n",
    "    Brand VARCHAR(255),\n",
    "    Color VARCHAR(255),\n",
    "    Unit_Cost_USD DECIMAL(10, 2),\n",
    "    Unit_Price_USD DECIMAL(10, 2),\n",
    "    Subcategory_Key INT,\n",
    "    Subcategory VARCHAR(255),\n",
    "    CategoryKey INT,\n",
    "    Category VARCHAR(255)\n",
    ")\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO products (Product_Key, Product_Name, Brand, Color, Unit_Cost_USD, Unit_Price_USD, Subcategory_Key, Subcategory, CategoryKey, Category)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame to list of tuples for insertion\n",
    "data_to_insert = Products_df.values.tolist()\n",
    "\n",
    "# Execute insertion\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "\n",
    "# Commit and close\n",
    "con.commit()\n",
    "cursor.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customers table\n",
    "import mysql.connector\n",
    "\n",
    "# Establishing connection to MySQL\n",
    "con = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"MynameisPBL@2710\",\n",
    "    database=\"DataSpark\"\n",
    ")\n",
    "cursor = con.cursor()\n",
    "\n",
    "# Creating the customers table\n",
    "query = \"\"\"CREATE TABLE IF NOT EXISTS customers (\n",
    "    Customer_Key INT,\n",
    "    Gender VARCHAR(50),\n",
    "    Name VARCHAR(255),\n",
    "    City VARCHAR(255),\n",
    "    State_Code VARCHAR(50),\n",
    "    State VARCHAR(255),\n",
    "    Zip_Code VARCHAR(50),\n",
    "    Country VARCHAR(255),\n",
    "    Continent VARCHAR(255),\n",
    "    Birthday DATE\n",
    ")\"\"\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Inserting data into customers table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO customers (Customer_Key, Gender, Name, City, State_Code, State, Zip_Code, Country, Continent, Birthday)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame to list of tuples for insertion\n",
    "data_to_insert = Customers_df.values.tolist()\n",
    "\n",
    "# Execute the insertion\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "con.commit()\n",
    "cursor.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores table\n",
    "import mysql.connector\n",
    "\n",
    "# Establishing connection to MySQL\n",
    "con = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"MynameisPBL@2710\",\n",
    "    database=\"DataSpark\"\n",
    ")\n",
    "cursor = con.cursor()\n",
    "\n",
    "# Creating the stores table\n",
    "query = \"\"\"CREATE TABLE IF NOT EXISTS stores (\n",
    "    Store_Key INT,\n",
    "    Country VARCHAR(255),\n",
    "    State VARCHAR(255),\n",
    "    Square_Meters FLOAT,\n",
    "    Open_Date DATE\n",
    ")\"\"\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Inserting data into stores table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO stores (Store_Key, Country, State, Square_Meters, Open_Date)\n",
    "VALUES (%s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame to list of tuples for insertion\n",
    "data_to_insert = Stores_df.values.tolist()\n",
    "\n",
    "# Execute the insertion\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "con.commit()\n",
    "cursor.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exchange_rates table\n",
    "import mysql.connector\n",
    "\n",
    "# Establish the connection\n",
    "con = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"MynameisPBL@2710\",\n",
    "    database=\"DataSpark\"\n",
    ")\n",
    "cursor = con.cursor()\n",
    "\n",
    "# Create the exchange_rates table\n",
    "query = \"\"\"CREATE TABLE IF NOT EXISTS exchange_rates (\n",
    "    Date DATE,\n",
    "    Currency VARCHAR(10),\n",
    "    Exchange FLOAT\n",
    ")\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "\n",
    "# Prepare the insert query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO exchange_rates (Date, Currency, Exchange)\n",
    "VALUES (%s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame to list of tuples for insertion\n",
    "data_to_insert = Exchange_rates_df.values.tolist()\n",
    "\n",
    "# Execute the insertion\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "\n",
    "# Commit the transaction and close con\n",
    "con.commit()\n",
    "cursor.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('customers',)\n",
      "('exchange_rates',)\n",
      "('products',)\n",
      "('sales',)\n",
      "('stores',)\n"
     ]
    }
   ],
   "source": [
    "#Check all the tables are created or not\n",
    "import mysql.connector\n",
    "\n",
    "con = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"MynameisPBL@2710\",\n",
    "    database=\"DataSpark\"\n",
    ")\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"show tables\")\n",
    "for i in cursor:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
